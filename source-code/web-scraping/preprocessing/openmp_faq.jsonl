{"input": "How old is the OpenMP ARB ?", "output": "The OpenMP ARB was founded in 1997. It celebrated its 20th birthday at the SC17 conference in Denver. The ARB is composed of ARB Members, ARB Advisors, and ARB Contributors. The list of all organizations affiliated with the OpenMP ARB can be found here. One of the best reasons for joining the OpenMP ARB is the OpenMP API itself-an important industry standard with a commitment to innovate within the industry. Members of the OpenMP ARB enjoy the following benefits: The OpenMP ARB today has the following subcommittees: For membership in the OpenMP ARB, your organization pays a one-time initiation fee plus an annual dues. The amount of the initiation fee equals the amount of the annual dues, differing by membership level, as follows: In addition to this financial cost, Language Committee members are encouraged to attend weekly Language Committee teleconferences and optional subgroup teleconferences, travel to three face-to-face meetings per year, prepare presentations, collaborate, and contribute work to further the group’s technical documents. The face-to-face meeting venues are chosen based on the location of OpenMP members, users, researchers, and communities. As OpenMP is a global community, these meetings are distributed globally: at this time one is in Europe and another is in North America. As the global membership increases, we expect to host meetings in Asia, Oceania, and South America. When you join as a member, you can invite OpenMP to your location and host a meeting of the OpenMP experts at a face-to-face meeting, while demonstrating your product, facility, and local hospitality. Contact the OpenMP ARB at the e-mail address [email protected] to obtain a membership form. The OpenMP ARB can be contacted in several ways. For general information, including other means of contacting the ARB, please see OpenMP’s website at: https://www.openmp.org/ General questions can be emailed to: [email protected] Technical Questions can be posted on the OpenMP Forum. You need to register on this Forum. The Forum has the following discussions: The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Who is affiliated with the OpenMP ARB ?", "output": "The ARB is composed of ARB Members, ARB Advisors, and ARB Contributors. The list of all organizations affiliated with the OpenMP ARB can be found here. One of the best reasons for joining the OpenMP ARB is the OpenMP API itself-an important industry standard with a commitment to innovate within the industry. Members of the OpenMP ARB enjoy the following benefits: The OpenMP ARB today has the following subcommittees: For membership in the OpenMP ARB, your organization pays a one-time initiation fee plus an annual dues. The amount of the initiation fee equals the amount of the annual dues, differing by membership level, as follows: In addition to this financial cost, Language Committee members are encouraged to attend weekly Language Committee teleconferences and optional subgroup teleconferences, travel to three face-to-face meetings per year, prepare presentations, collaborate, and contribute work to further the group’s technical documents. The face-to-face meeting venues are chosen based on the location of OpenMP members, users, researchers, and communities. As OpenMP is a global community, these meetings are distributed globally: at this time one is in Europe and another is in North America. As the global membership increases, we expect to host meetings in Asia, Oceania, and South America. When you join as a member, you can invite OpenMP to your location and host a meeting of the OpenMP experts at a face-to-face meeting, while demonstrating your product, facility, and local hospitality. Contact the OpenMP ARB at the e-mail address [email protected] to obtain a membership form. The OpenMP ARB can be contacted in several ways. For general information, including other means of contacting the ARB, please see OpenMP’s website at: https://www.openmp.org/ General questions can be emailed to: [email protected] Technical Questions can be posted on the OpenMP Forum. You need to register on this Forum. The Forum has the following discussions: The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What are the benefits of joining the OpenMP ARB?", "output": "One of the best reasons for joining the OpenMP ARB is the OpenMP API itself-an important industry standard with a commitment to innovate within the industry. Members of the OpenMP ARB enjoy the following benefits: The OpenMP ARB today has the following subcommittees: For membership in the OpenMP ARB, your organization pays a one-time initiation fee plus an annual dues. The amount of the initiation fee equals the amount of the annual dues, differing by membership level, as follows: In addition to this financial cost, Language Committee members are encouraged to attend weekly Language Committee teleconferences and optional subgroup teleconferences, travel to three face-to-face meetings per year, prepare presentations, collaborate, and contribute work to further the group’s technical documents. The face-to-face meeting venues are chosen based on the location of OpenMP members, users, researchers, and communities. As OpenMP is a global community, these meetings are distributed globally: at this time one is in Europe and another is in North America. As the global membership increases, we expect to host meetings in Asia, Oceania, and South America. When you join as a member, you can invite OpenMP to your location and host a meeting of the OpenMP experts at a face-to-face meeting, while demonstrating your product, facility, and local hospitality. Contact the OpenMP ARB at the e-mail address [email protected] to obtain a membership form. The OpenMP ARB can be contacted in several ways. For general information, including other means of contacting the ARB, please see OpenMP’s website at: https://www.openmp.org/ General questions can be emailed to: [email protected] Technical Questions can be posted on the OpenMP Forum. You need to register on this Forum. The Forum has the following discussions: The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Which subcommittees are there in the OpenMP ARB ?", "output": "The OpenMP ARB today has the following subcommittees: For membership in the OpenMP ARB, your organization pays a one-time initiation fee plus an annual dues. The amount of the initiation fee equals the amount of the annual dues, differing by membership level, as follows: In addition to this financial cost, Language Committee members are encouraged to attend weekly Language Committee teleconferences and optional subgroup teleconferences, travel to three face-to-face meetings per year, prepare presentations, collaborate, and contribute work to further the group’s technical documents. The face-to-face meeting venues are chosen based on the location of OpenMP members, users, researchers, and communities. As OpenMP is a global community, these meetings are distributed globally: at this time one is in Europe and another is in North America. As the global membership increases, we expect to host meetings in Asia, Oceania, and South America. When you join as a member, you can invite OpenMP to your location and host a meeting of the OpenMP experts at a face-to-face meeting, while demonstrating your product, facility, and local hospitality. Contact the OpenMP ARB at the e-mail address [email protected] to obtain a membership form. The OpenMP ARB can be contacted in several ways. For general information, including other means of contacting the ARB, please see OpenMP’s website at: https://www.openmp.org/ General questions can be emailed to: [email protected] Technical Questions can be posted on the OpenMP Forum. You need to register on this Forum. The Forum has the following discussions: The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What are the membership levels and cost of membership ?", "output": "For membership in the OpenMP ARB, your organization pays a one-time initiation fee plus an annual dues. The amount of the initiation fee equals the amount of the annual dues, differing by membership level, as follows: In addition to this financial cost, Language Committee members are encouraged to attend weekly Language Committee teleconferences and optional subgroup teleconferences, travel to three face-to-face meetings per year, prepare presentations, collaborate, and contribute work to further the group’s technical documents. The face-to-face meeting venues are chosen based on the location of OpenMP members, users, researchers, and communities. As OpenMP is a global community, these meetings are distributed globally: at this time one is in Europe and another is in North America. As the global membership increases, we expect to host meetings in Asia, Oceania, and South America. When you join as a member, you can invite OpenMP to your location and host a meeting of the OpenMP experts at a face-to-face meeting, while demonstrating your product, facility, and local hospitality. Contact the OpenMP ARB at the e-mail address [email protected] to obtain a membership form. The OpenMP ARB can be contacted in several ways. For general information, including other means of contacting the ARB, please see OpenMP’s website at: https://www.openmp.org/ General questions can be emailed to: [email protected] Technical Questions can be posted on the OpenMP Forum. You need to register on this Forum. The Forum has the following discussions: The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How do I join the OpenMP ARB?", "output": "Contact the OpenMP ARB at the e-mail address [email protected] to obtain a membership form. The OpenMP ARB can be contacted in several ways. For general information, including other means of contacting the ARB, please see OpenMP’s website at: https://www.openmp.org/ General questions can be emailed to: [email protected] Technical Questions can be posted on the OpenMP Forum. You need to register on this Forum. The Forum has the following discussions: The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How do I contact the OpenMP ARB for more information ?", "output": "The OpenMP ARB can be contacted in several ways. For general information, including other means of contacting the ARB, please see OpenMP’s website at: https://www.openmp.org/ General questions can be emailed to: [email protected] Technical Questions can be posted on the OpenMP Forum. You need to register on this Forum. The Forum has the following discussions: The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Where do I find more information on the OpenMP ARB ?", "output": "The website https://www.openmp.org/ contains information on membership, specifications, books, tutorials, compilers, users, and more. The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What is IWOMP ?", "output": "The International Workshop on OpenMP (IWOMP) is an annual workshop dedicated to the promotion and advancement of all aspects of parallel programming with OpenMP. It is the premier forum to present and discuss issues, trends, recent research ideas and results related to parallel programming with OpenMP. The international workshop affords an opportunity for OpenMP users as well as developers to come together for discussions and sharing new ideas and information on this topic. The location of the workshop rotates between the USA, Europe and Asia. IWOMP is held annually in September. This means that you can expect the Call for Proposals in February and the paper acceptance in June. The website of the most recent IWOMP can be viewed at www.iwomp.org. OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What is OpenMPCon ?", "output": "OpenMPCon, the OpenMP Developers Conference, is the annual forum for the discussion of all aspects of parallel programming with OpenMP. It is a user-focused conference with tutorials and sessions to help users further their knowledge of OpenMP. Attendees of OpenMPCon attend talks, presentations, and panels with some of the worldâ€™s leading experts in OpenMP. The location of OpenMPCon rotates between the USA, Europe, and Asia and immediately precedes IWOMP. The conference is held annually in September, with the Call for Proposals usually opening in May with paper acceptance in June. The website for OpenMPCon can be viewed at openmpcon.org.  OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What is OpenMP ?", "output": "OpenMP is a specification for a set of compiler directives, library routines, and environment variables that can be used to specify high-level parallelism in Fortran and C/C++ programs. In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What problem does OpenMP solve ?", "output": "In order to parallelize a code, programmers look for regions of code whose instructions can be shared among the processors. Much of the time, they focus on distributing the work in loop nests to the processors. In most programs, code executed on one processor require results that have been calculated on another one. In principle, this is not a problem because a value produced by one processor can be stored in main memory and retrieved from there by code running on other processors as needed. However, the programmer needs to ensure that the value is retrieved after it has been produced, that is, that the accesses occur in the required order. Since the processors operate independently of one another, this is a nontrivial difficulty: their clocks are not synchronized, and they can and do execute their portions of the code at slightly different speeds. To solve this problem, the vendors of SMPs in the 1980s provided special notation to specify how the work of a program was to be parceled out to the individual processors of an SMP, as well as to enforce an ordering of accesses by different threads to shared data. The notation mainly took the form of special instructions, or directives, that could be added to programs written in sequential languages, especially Fortran. The compiler used this information to create the actual code for execution by each processor. Although this strategy worked, it had the obvious deficiency that a program written for one SMP did not necessarily execute on another one. In order to solve this problem of non-standardization, OpenMP was defined by the OpenMP ARB, a group of vendors who joined forces during the latter half of the 1990s to provide a common means for programming a broad range of SMP architectures. The first version, consisting of a set of directives that could be used with Fortran, was introduced to the public in late 1997. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) With the following versions, OpenMP was extended so that also DSP systems and accelerators could be programmed. There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Why should I use OpenMP ?", "output": "There are several reasons why you should use OpenMP: (a) OpenMP is the most widely standard for SMP systems, it supports 3 different languages (Fortran, C, C++), and it has been implemented by many vendors. (b) OpenMP is a relatively small and simple specification, and it supports incremental parallelism. (c) A lot of research is done on OpenMP, keeping it up to date with the latest hardware developments. A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Which compilers support OpenMP ?", "output": "A number of compilers from various vendors or open source communities implement the OpenMP API. The full list of compilers can be found here. The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Who uses OpenMP ?", "output": "The users of OpenMP are working in industry and academia, in fields varying from aeronautics, automotive, pharmaceutics to finance, and on devices varying from accelerators, embedded multicore systems to high-end supercomputing systems. cOMPunity is the community of OpenMP researchers and developers in academia and industry. It is a forum for the dissemination and exchange of information about OpenMP. The OpenMP ARB has also started compiling a list of users of OpenMP. OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What languages does OpenMP support ?", "output": "OpenMP is designed for Fortran, C and C++. OpenMP can be supported by compilers that support one of Fortran 77, Fortran 90, Fortran 95, Fortran 2003, Fortran 2008, C11, C++11, and C++14, but the OpenMP specification does not introduce any constructs that require specific Fortran 90 or C++ features OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Is OpenMP scalable ?", "output": "OpenMP can deliver scalability for applications using shared-memory parallel programming. Significant effort was spent to ensure that OpenMP can be used for scalable applications. However, ultimately, scalability is a property of the application and the algorithms used. The parallel programming language can only support the scalability by providing constructs that simplify the specification of the parallelism and can be implemented with low overhead by compiler vendors. OpenMP certainly delivers these kinds of constructs. You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Where can I get tutorials on using the API ?", "output": "You can find an introductory tutorial on the OpenMP  YouTube channel. You can also find lists of tutorials on the OpenMP web site. There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Where do I find more information on using the API ?", "output": "There are several sources of information on using the API: OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Can I use loop-level parallelism ?", "output": "OpenMP fully supports loop-level parallelism. Loop-level parallelism is useful for applications which have lots of coarse loop-level parallelism, especially those that will never be run on large numbers of processors or for which restructuring the source code is either impractical or disallowed. Typically, though, the amount of loop-level parallelism in an application is limited, and this in turn limits the scalability of the application. OpenMP allows you to use loop-level parallelism as a way to start scaling your application for multiple processors, but then move into coarser grain parallelism, while maintaining the value of your earlier investment. This incremental development strategy avoids the all-or-none risks involved in moving to message-passing or other parallel programming models. If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Can I use nested parallelism ?", "output": "If a thread in a team executing a parallel region encounters another parallel construct, it creates a new team and becomes the master of that team. This is generally referred to in OpenMP as nested parallelism, and it is supported by the OpenMP specification. Certain recursive algorithms can take advantage of nested parallelism in a natural way. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Can I use task parallelism ?", "output": "Parallelization strategies that parcel out pieces of work to different threads are generally referred to as task parallelism. This is supported by the OpenMP specification. (Quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Is it better to parallelize the outer loop ?", "output": "In general it should be fastest to parallelize the outer loop only, provided there is sufficient parallelism to keep the threads busy and load balanced. Parallelizing the inner loop only adds an overhead for every parallel region encountered which (although dependent on the implementation and the number of threads) is typically of the order of tens of microseconds. Parallelizing both loops is unlikely to be the most efficient solution, except maybe in some corner cases. Sometimes, if the outer loop doesn’t have much parallelism but the inner loop does, there is the option to put the #pragma omp parallel outside the outer loop, and the #pragma omp for before the inner loop. This amortizes the overhead of creating the parallelism outside the outer loop, runs the outer loop ‘redundantly’ on all threads, and work-shares the iterations of the inner loop, which can be the better than parallelizing either the outer or the inner loop alone. OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Can I use OpenMP to program accelerators ?", "output": "OpenMP provides mechanisms to describe regions of code where data and/or computation should be moved to another computing device. OpenMP supports a broad array of accelerators, and plans to continue adding new features based on user feedback. OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "Can I use OpenMP to program SIMD units ?", "output": "OpenMP offers an industry-first high-level support for vectors that allows you to parallelize vector computations. A loop can be transformed into a SIMD loop (that is, multiple iterations of the loop can be executed concurrently using SIMD vector instructions). It has specific constructs to specify that during execution of functions (C, C++ and Fortran) or subroutines (Fortran) specialized kernel functions are sent to the vector unit. OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "What type of reductions are possible ?", "output": "OpenMP provides the reduction clause for specifying some form of recurrence calculations so that they can be perfomed in parallel without code modification. In OpenMP 3.1, OpenMP supports reductions with base language operators and intrinsic procedures. With OpenMP 4.0, user-defined reductions are also supported. (The first sentence is a quote from Using OpenMP: Portable Shared Memory Parallel Programming, by Barbara Chapman, Gabriele Jost and Ruud van der Pas, MIT Press, Cambridge, MA, 2008) OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How does Version 4.5 of OpenMP differ from Version 4.0 ?", "output": "OpenMP 4.5 was released in November 2015, 2 years after the release of OpenMP 4.0. The new features included in OpenMP 4.5 are: OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How does Version 4.0 of OpenMP differ from Version 3.1 ?", "output": "OpenMP 4.0 was released in July 2013, 2 years after the release of OpenMP version 3.1. The new features included in OpenMP 4.0 are: OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How did Version 3.1 of OpenMP differ from Version 3.0 ?", "output": "OpenMP version 3.1 was released in July 2011, 3 years after the release of OpenMP version 3.0. The major changes between these versions are: OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How did Version 3.0 of OpenMP differ from Version 2.5 ?", "output": "OpenMP version 3.0 was released in May 2008, 3 years after the release of OpenMP version 2.5. The major changes between these versions are: OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How does OpenMP relate to OpenACC ?", "output": "OpenMP and OpenACC are actively merging their specification while continuing to evolve. A first step at merging has been made with the release of OpenMP 4.0. OpenACC implementations can be considered to be a beta test of the OpenMP accelerator specification. They give early implementation experience. OpenACC has been created and implemented by several members of the OpenMP ARB in order to address their immediate customer needs. These members are NVIDIA, PGI, Cray, and CAPS. Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How does OpenMP compare with MPI ?", "output": "Message-passing has become accepted as a portable style of parallel programming, but has several significant weaknesses that limit its effectiveness and scalability. Message-passing in general is difficult to program and doesn’t support incremental parallelization of an existing sequential program. Message-passing was initially defined for client/server applications running across a network, and so includes costly semantics (including message queuing and selection and the assumption of wholly separate memories) that are often not required by tightly-coded scientific applications running on modern scalable systems with globally addressable and cache coherent distributed memories. Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How does OpenMP compare with Pthreads ?", "output": "Pthreads have never been targeted toward the technical/HPC market. This is reflected in the minimal Fortran support, and its lack of support for data parallelism. Even for C applications, pthreads requires programming at a level lower than most technical developers would prefer. The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
{"input": "How does OpenMP compare with MIPI ?", "output": "The Mobile Industry Processor Interface (MIPI) is addressing a range of debug interface efforts for multi-core devices. However, its specifications are focused on mobile devices and not multi-core processors in general."}
